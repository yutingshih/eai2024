
@misc{liu_post-training_2021,
	title = {Post-Training Quantization for Vision Transformer},
	url = {http://arxiv.org/abs/2106.14156},
	doi = {10.48550/arXiv.2106.14156},
	abstract = {Recently, transformer has achieved remarkable performance on a variety of computer vision applications. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an effective post-training quantization algorithm for reducing the memory storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the optimal low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, we thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-training quantization algorithms. For instance, we can obtain an 81.29{\textbackslash}\% top-1 accuracy using {DeiT}-B model on {ImageNet} dataset with about 8-bit quantization.},
	number = {{arXiv}:2106.14156},
	publisher = {{arXiv}},
	author = {Liu, Zhenhua and Wang, Yunhe and Han, Kai and Ma, Siwei and Gao, Wen},
	urldate = {2024-03-18},
	date = {2021-06-27},
	eprinttype = {arxiv},
	eprint = {2106.14156 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hsu_deep_2020,
	title = {Deep Fake Image Detection Based on Pairwise Learning},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/1/370},
	doi = {10.3390/app10010370},
	abstract = {Generative adversarial networks ({GANs}) can be used to generate a photo-realistic image from a low-dimension random noise. Such a synthesized (fake) image with inappropriate content can be used on social media networks, which can cause severe problems. With the aim to successfully detect fake images, an effective and efficient image forgery detector is necessary. However, conventional image forgery detectors fail to recognize fake images generated by the {GAN}-based generator since these images are generated and manipulated from the source image. Therefore, in this paper, we propose a deep learning-based approach for detecting the fake images by using the contrastive loss. First, several state-of-the-art {GANs} are employed to generate the fakeâ€“real image pairs. Next, the reduced {DenseNet} is developed to a two-streamed network structure to allow pairwise information as the input. Then, the proposed common fake feature network is trained using the pairwise learning to distinguish the features between the fake and real images. Finally, a classification layer is concatenated to the proposed common fake feature network to detect whether the input image is fake or real. The experimental results demonstrated that the proposed method significantly outperformed other state-of-the-art fake image detectors.},
	pages = {370},
	number = {1},
	journaltitle = {Applied Sciences},
	author = {Hsu, Chih-Chung and Zhuang, Yi-Xiu and Lee, Chia-Yen},
	urldate = {2024-03-18},
	date = {2020-01},
	langid = {english},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {{GAN}, contrastive loss, deep learning, forgery detection, pairwise learning.},
}

@misc{zhao_atom_2023,
	title = {Atom: Low-bit Quantization for Efficient and Accurate {LLM} Serving},
	url = {http://arxiv.org/abs/2310.19102},
	doi = {10.48550/arXiv.2310.19102},
	shorttitle = {Atom},
	abstract = {The growing demand for Large Language Models ({LLMs}) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for {LLM} service providers. To efficiently use {GPU} resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, {LLM} quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern {GPUs}, such as 4-bit integer operators, resulting in sub-optimal performance. To maximize {LLMs}' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-activation quantization setups in the serving context. Atom improves end-to-end throughput by up to \$7.73{\textbackslash}times\$ compared to the {FP}16 and by \$2.53{\textbackslash}times\$ compared to {INT}8 quantization, while maintaining the same latency target.},
	number = {{arXiv}:2310.19102},
	publisher = {{arXiv}},
	author = {Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
	urldate = {2024-01-29},
	date = {2023-11-07},
	eprinttype = {arxiv},
	eprint = {2310.19102 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{xiong_layer_2020,
	title = {On Layer Normalization in the Transformer Architecture},
	url = {http://arxiv.org/abs/2002.04745},
	doi = {10.48550/arXiv.2002.04745},
	abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-{LN} Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-{LN} Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-{LN} Transformers. We show in our experiments that Pre-{LN} Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
	number = {{arXiv}:2002.04745},
	publisher = {{arXiv}},
	author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan},
	urldate = {2024-03-18},
	date = {2020-06-29},
	eprinttype = {arxiv},
	eprint = {2002.04745 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{shen_powernorm_2020,
	title = {{PowerNorm}: Rethinking Batch Normalization in Transformers},
	url = {http://arxiv.org/abs/2003.07845},
	doi = {10.48550/arXiv.2003.07845},
	shorttitle = {{PowerNorm}},
	abstract = {The standard normalization method for neural network ({NN}) models used in Natural Language Processing ({NLP}) is layer normalization ({LN}). This is different than batch normalization ({BN}), which is widely-adopted in Computer Vision. The preferred use of {LN} in {NLP} is principally due to the empirical observation that a (naive/vanilla) use of {BN} leads to significant performance degradation for {NLP} tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of {NLP} transformer models to understand why {BN} has a poor performance, as compared to {LN}. We find that the statistics of {NLP} data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if {BN} is naively implemented. To address this, we propose Power Normalization ({PN}), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in {BN}, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that {PN} leads to a smaller Lipschitz constant for the loss, compared with {BN}. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test {PN} for transformers on a range of {NLP} tasks, and we show that it significantly outperforms both {LN} and {BN}. In particular, {PN} outperforms {LN} by 0.4/0.6 {BLEU} on {IWSLT}14/{WMT}14 and 5.6/3.0 {PPL} on {PTB}/{WikiText}-103. We make our code publicly available at {\textbackslash}url\{https://github.com/{sIncerass}/powernorm\}.},
	number = {{arXiv}:2003.07845},
	publisher = {{arXiv}},
	author = {Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
	urldate = {2024-03-18},
	date = {2020-06-28},
	eprinttype = {arxiv},
	eprint = {2003.07845 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-03-16},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	number = {{arXiv}:2010.11929},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2024-03-15},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, {ViT}, Vision Transformer},
}
